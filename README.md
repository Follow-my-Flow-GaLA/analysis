# Follow My Flow: Unveiling Client-Side Prototype Pollution Gadgets from One Million Real-World Websites

In this paper, we design a dynamic analysis framework, called GaLA (<u>Ga</u>dget <u>L</u>ocator and <u>A</u>nalyzer)
, to automatically detect client-side prototype pollution gadgets among real-world websites, and implement an open-source version of GaLA. Our key insight is to borrow existing defined values on non-vulnerable websites to victim ones where such values are undefined, thus guiding the property injection to flow to the sinks in gadgets. 

Our paper has been accepted by [IEEE Security & Privacy 2025](https://sp2025.ieee-security.org/accepted-papers.html) and is accessible via [this link](https://www.yinzhicao.org/ProbetheProto/FollowMyFlow.pdf). 

## System Design Overview

<div style="text-align: center;">
    <img src="./pic/gadget-sys-arch-2-1.png" alt="Figure 1: System Architecture" width="750">
</div>


System Architecture of GALA. There are three major phases: (i) Locating undefined properties, (ii) Assigning defined values, and (iii) Guiding dataflows for originally undefined properties. In Phase 1, GALA runs an instrumented JS runtime to output all the undefined properties; in Phase 2, GALA finds corresponding defined values in other executions (which could exist in the same or a different website) and assigns such values to undefined ones in Phase 1; in Phase 3, GALA uses these defined values to guide the execution with previously undefined values to reach the sink and discover gadgets. All the gadgets are validated automatically using a generated payload to ensure corresponding consequences are achieved.


## Artifacts Overview

The artifacts in this repo comprise two main components: the adapted taint engine based on Chromium V8, and Python codes to analyze the log files generated by the modified engine. 

### Adapted Taint Engine

The engine used by GaLA is adapted from prior taint engines [Melicher et al.](https://github.com/wrmelicher/ChromiumTaintTracking) and [ProbetheProto](https://github.com/zifeng-kang/ProbetheProto). 

**Step 1: Building Official Chromium and Installing Dependencies**

Please follow instructions in [Melicher et al.](https://github.com/wrmelicher/ChromiumTaintTracking/blob/master/TAINT_TRACKING_README) to build a modified Chromium and install Capnp for encoding the taint byte information. Remember to install all the dependencies specified in that instruction. 

**Important Notes:**

1. Some version information: (i) My VM is running an old version of Ubuntu "16.04.7 LTS (Xenial Xerus)". Ubuntu 14.04 is also fine. Newer version may be incompatible with the taint engine because it is built on Chromium 54.0.2822.0, a very old version; (ii) `gclient` should be checked out using `gclient checkout 8d0c21dddbe95f83dc7323d749a9bcff9a84e020`; (iii) The libgcrypt deb file is `libgcrypt11_1.5.3-2ubuntu4_amd64.deb`. You can search for it, download it and then run `sudo dpkg -i <path_to_that_deb>/libgcrypt11_1.5.3-2ubuntu4_amd64.deb`. 

2. **Disclaimer:** This engine is intended strictly for academic use. It is built upon an outdated version of Chromium and runs without sandboxing, and we therefore provide NO assurances regarding its security or privacy. 

After installation, you can start with building an official Chromium. For example, building into `Official` dir requires: 
`mkdir out/Official` in the directory where you downloaded source codes for Chromium; 
Then, write proper configurations into `out/Official/args.gn`, and remember to change `"/media/data1/zfk/` to your own path: 
```
cat <<EOF > out/Official/args.gn
v8_interpreted_regexp = true

is_component_build = true

symbol_level = 0
enable_nacl = false
remove_webcore_debug_symbols = true
is_debug = false

v8_capnp_include_dir = "/media/data1/zfk/Documents/capnproto-install"
EOF
```
And finally build Chromium using ninja: `<path_to>/depot_tools/ninja -C out/Official chrome`

**Step 2: Building Modified Chromium**

On the codebase of the official Chromium, you may apply the patches by Melicher et al. (chromium_patch and v8_patch) and build the Chromium again. Note that you may encounter lots of dependency errors; just solve them one by one, potentially with the help from LLM, and eventually you will get through all of those annoying errors! For example, you can install missing packages if ninja is complaining about them; or you can create symbolic links for a specific file when ninja says no symbolic link is found. We are also happy to help you with the dependency issues. 

To apply the modifications by GaLA, you can either (i) pull the codes from [our V8 repo for GaLA]([https://github.com/Follow-my-Flow-GaLA/v8]) or, if that does not work, (ii) search for '// Add by Inactive' in that repo and manually apply corresponding codes. Solution (ii) is also a convenient way to understand what modifications we have made for GaLA to work! 

A (mostly comprehensive) list of files modified by GaLA: v8/src/objects.cc, v8/src/objects.h, v8/src/objects-inl.h, v8/src/runtime/runtime-object.cc & .h, v8/src
/flag-definitions.h, v8/src/taint_tracking/protos/logrecord.capnp, v8/include/v8.h (this file must be kept in sync with ../../third_party/WebKit/Source/wtf/text/TaintTracking.h) and adding v8/src/SHA256.cc & .h. Note: In `v8/include/v8.h` you may also find the taint byte design by Melicher et al.. 

**Step 3: Running Modified Chromium for Phases 1, 2 and 3**

After successfully building the Chromium into different repos, e.g., `out/Detector-release/` for Phase 1 in our codebase and `out/Oracle-release/` for Phase 2 and 3, you can run the Chromium for each phase. You don't need to change anything before building them into different repos, because each phase is controlled by different flags during runtime, i.e., `--inactive_conseq_log_enable` for Phase 1, `--inactive_taint_enable` for Phase 2, and `--phase3_enable` for Phase 3. 

Before running each phase, a mongodb server should be setup and running. We put these steps in "Phase 1: Locating Undefined Properties" in the "Log Analysis" section; please check them below. 

To conduct large-scale crawling, cd to `sanchecker/src`: 

* Phase 1: Check the third line of `detector-phase1-auto-recursive.sh` and run that long command begnning with `sudo`. Remember to make sure the mongodb server is active. 
* Phase 2: Check the third line of `oracle-phase2-auto-recursive.sh` and run it, with active mongodb.
* Phase 3: Check the third line of `detector-phase3-auto-recursive.sh` and run it, with active mongodb. (If you would like to run Phase 3 recursively to discover more undefined properties for chained gadgets, please also check the `oracle-phase4-auto-recursive.sh`. That so-called phase 4 file is helpful for recursively running phase 3, but does not indicate we have a 'phase 4'. )

**Step 4: Post-processing the Logs**

The crawling generates two types of log files: (i) Files with suffix `_log_file`. They are generated by the modified Chromium and are useful for subsequent phases. For example, Phase 2 will need undefined properties from Phase 1; (ii) Files in dir `*_crawl`, whose names have a format of `<domain_name>_<some_numbers>` and whose contents are encoded taint byte information (Note that Phase 1 does not necessarily need these information). They should be decoded first. Steps to decode them:

1. cd to `./sanchecker/`, check `filter_filename_from_ls.py`, change `DB_PATH` and `PREFIX` to your own paths, and execute `python3 filter_filename_from_ls.py`;
2. Check `decode_capnp_.sh`, change `TARGET_STR` to your own name for the crawling, e.g., `"detector_1M_phase3_db_"` is for Phase 3. Change `/media/data1/zfk/Documents/capnproto-install/bin/capnp` and `/media/data1/zfk/Documents/sanchecker/src/v8/src/taint_tracking/protos/logrecord.capnp` to your own paths. 
3. Run `sudo ./decode_capnp_.sh` to decode the information. The results will be stored in files with prefix `record_` in the `./sanchecker/*_crawl` dir. 

### Log Analysis

The `analysis` directory contains various files, but not all are necessary for the main pipelineâ€”some are used only for debugging or statistics. Below are instructions for using the relevant scripts for each phase of GaLA.

#### Phase 1: Locating Undefined Properties

**Prerequisites:**

1. Run the Adapted Taint Engine for Phase 1 to generate site logs with undefined property information.
2. Set up a local MongoDB server. This includes installing `mongod`, checking the instructions written in `analysis/db/app.py` (Lines 6-16), and following the instructions. Note that `flask run &>> flask.log` is for Phases 1 & 2; `flask run --cert=$FLASK_RUN_CERT --key=$FLASK_RUN_KEY &>> flask.log` is for Phase 3, because Phase 3 needs HTTPS. 

**Steps:**

1. Modify the `base_dirs` variable in `analysis/phase1/save_log_to_db_offline.py` to point to the directory containing Phase 1 logs generated by the Adapted Taint Engine.
2. Run `save_log_to_db_offline.py` to save Phase 1 information into the MongoDB database.

#### Phase 2: Assigning Defined Values

**Prerequisites:**

1. Run the Adapted Taint Engine for Phase 2 to generate site logs and a record of defined values corresponding to undefined properties (which may be from the same or a different website).
2. Ensure your MongoDB server is running, i.e., the `status` is `active`.

**Steps:**

1. Update `PHASE2_RECORD_PATH` and `PHASE2_LOG_PATH` in `analysis/phase2/config.py` to reflect the locations of the Phase 2 logs and records.
2. Run `analysis/phase2/gen_phase2_db.py` to match dataflows in the logs with the records and identify flows reaching the sink. Both matched and unmatched flows will be stored in the database.
3. Update the `csv_file_path` in `analysis/phase2/strict_match/strict_match.py` to point to the file containing the list of all site names.
4. Run `strict_match.py` to assign defined values from Phase 2 to the undefined properties from Phase 1. The property-value pairs will be stored in the database for use in Phase 3 by the Adapted Taint Engine.

#### Phase 3: Validating Flows and Exploit Generation

**Part 1: Flow Validation and Exploit Generation**

**Prerequisites:**  
Use the defined property-value pairs to guide execution with previously undefined values.

**Steps:**
1. Modify the necessary file paths in `analysis/phase3/phase3_config.py`.
2. Add your OpenAI API key to `analysis/phase3/exploit_gen/llm_config.py` if you would like to use llm-based exploit generator.
3. Run `analysis/phase3/count_gadgets_phase3_db.py` to generate exploits for validated flows.

**Part 2: Gadget Validation**

**Prerequisite:**  
Inject the generated exploits into the targeted sites.

**Steps:**
1. Run `analysis/phase3/check_def_flows.py` to verify if the exploits successfully reach the sink.
